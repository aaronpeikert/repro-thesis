\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[12pt,a4paper,]{article}
\usepackage{lmodern}
\usepackage{setspace}
\setstretch{1.5}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\usepackage{hyperref}
\hypersetup{
            pdftitle={Reproducibility made simple},
            pdfauthor={Aaron Peikert},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage[left=2.5cm, right=2cm]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{longtable}}{}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{booktabs}
\usepackage{libertine}
\usepackage{libertinust1math}
\usepackage{emptypage}
\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\floatpagefraction}{0.75}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother

\title{Reproducibility made simple}
\providecommand{\subtitle}[1]{}
\subtitle{Automating reproducible research workflows}
\author{Aaron Peikert}
\date{2020-05-22}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{2}
\tableofcontents
}
\hypertarget{abstract}{%
\section*{Abstract}\label{abstract}}
\addcontentsline{toc}{section}{Abstract}

This is a short summary.

\hypertarget{theoretical-considerations}{%
\section{Theoretical Considerations}\label{theoretical-considerations}}

Claerbout \& Karrenbach (\protect\hyperlink{ref-claerboutElectronicDocumentsGive1992}{1992}) define reproducibility as the ability to gain the same results, from the same dataset.
Conversely, they call a result replicable if one draws the same conclusion from a new dataset.
This thesis concerns itself with the former, providing researchers with an accessible analysis workflow, that is virtually guaranteed to reproduce across time and devices.
The scientific community agrees that their work should be ideally reproducible.
Indeed it may be hard to find a researcher who distrusts a result because it is reproducible; to the contrary, many feel it is ``good scientific practice'' to ensure it is (``Reducing our irreproducibility,'' \protect\hyperlink{ref-AnnouncementReducingOur2013}{2013}; Deutsche Forschungsgemeinschaft, \protect\hyperlink{ref-dfg2019}{2019}; Epskamp, \protect\hyperlink{ref-epskamp2019rep}{2019}).
Several reasons, practical and meta-scientific, justify this consensus of reproducibility as a minimal standard of Science.

Reproducibility makes researchers life more productive in two ways:
The act of reproduction provides, at the most basic level, an opportunity to spot errors, helping the researchers who originally produced them.
At the same time, other researchers may benefit from reusing materials from an analysis they reproduced.

Beyond these two purely pragmatic reasons, reproduction is crucial, depending on the philosophical view of Science one subscribes to, because it allows independent validation and enables replication.
Philosophers of Science characterise Science by a shared method of determining if a statement about the world is ``true'' (Andersen \& Hepburn, \protect\hyperlink{ref-andersonScientificMethod2016}{2016}) or more broadly evaluating the statements verisimilitude (Gilbert, \protect\hyperlink{ref-gilbertModelBuildingDefinition1991}{1991}; Meehl, \protect\hyperlink{ref-meehlAppraisingAmendingTheories1990}{1990}; Popper, \protect\hyperlink{ref-popperCommentsTruthGrowth1962}{1962}; Tichỳ, \protect\hyperlink{ref-tichyVerisimilitudeRedefined1976}{1976}).
If this method is for experts to agree on the assumptions and deduce some truth, reproducibility is hardly necessary.
On the other hand, it gains importance if one induces facts by carefully observing the world.
The decisive difference is that the former gains credibility through the authority of the experts, while the latter is trustworthy because anyone may verify it.
Accepting induction as a scientific method hence hinges on the verifiability by others.
Some have even argued that such democratisation of Science is what fueled the scientific revolution (Heilbron, \protect\hyperlink{ref-heilbronOxfordCompanionHistory2004}{2004}, Scientific Revolution).
The scientific revolution had the experiment as an agreed-upon method to observe the reality and a much later revolution provides statistical modelling (Rodgers, \protect\hyperlink{ref-rodgersEpistemologyMathematicalStatistical2010}{2010}) as a means to induction.
This consensus, about how to observe and how to induce, gives modern scientific enterprises much of its credibility.
Two reasons justify why we must assume reproducibility as a scientific standard if we accept induction as a scientific method:
First, it enables independent verification of the process of induction, and second, it dramatically simplifies replication as a means to verify the induced truths.

However, neither the practical reasons that results may be less error-prone and more reusable nor the meta-scientific grounds that the process of induction and the induced facts are more straightforward to verify, if reproducible, follow strictly from the definition on reproducibility provided by Claerbout \& Karrenbach (\protect\hyperlink{ref-claerboutElectronicDocumentsGive1992}{1992}) given above.
A simple thought experiment illustrates this shortcoming.
Imagine a binary program that is perfectly reproducible; hence upon input of the same dataset, it fills a scientific manuscript with the same numbers at the right places. Furthermore, assume this hypothetical program may never hold if the data changes.
Does the predicate ``reproducible'' in situation reduce the number of mistakes or enables reuse? Unlikely.
Or could one audit it and use it in replication? Hardly.
This admittedly constructed case of a reproducible black box shows us: we are not interested in reproducibility, we are interested in its side effects.

Spoiling its elegant simplicity, I change the definition by Claerbout \& Karrenbach (\protect\hyperlink{ref-claerboutElectronicDocumentsGive1992}{1992}) to address this issue, by further demanding that reproducibility must facilitate replication.
Hence, I would call a result only then reproducible if the results remain unchanged if the data does, and it furthermore helps other researchers to replicate the results if they attempt to do so.
With such a notion, the only valid cause of reproducibility is transparency.
Only if it is clear how the data relates to its results, both reproducibility and replication get promoted.
It follows that something is no longer either reproducible or not, but there are shades, because a research product may promote replication to varying degrees.
Note, that a scientific result can facilitate replication without anyone ever attempting to replicate it, e.g.~by educating other researches about the analyses method, being openly accessible and providing reusable components.

Hence reproducibility has a technical side, ensuring the same results, and a non-technical side, facilitating understanding.
The former relates to the practical advantages while the latter serves the metascientific purposes of reproducibility.
An important caveat of the technical aspect is that generating the same results from the same data should be possible regardless of time and machine.
Following a reproducible analysis should be:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  understandable by other researchers
\item
  transferable across machines
\item
  conserved through time.
\end{enumerate}

This much more demanding standard of reproducibility gains justification by two recent developments in the social sciences in general and psychology in particular: the emergence of a ``replication crises'' (Ioannidis, \protect\hyperlink{ref-ioannidisWhyMostPublished2005}{2005}) and the rise of ``machine learning'' (Jordan \& Mitchell, \protect\hyperlink{ref-jordanMachineLearningTrends2015}{2015}) as a scientific tool.
Both trends link to the use of statistical modelling on which the social sciences became reliant for testing and developing their theories (Gigerenzer et al., \protect\hyperlink{ref-gigerenzerNullRitualWhat2004}{2004}; Meehl, \protect\hyperlink{ref-meehlTheoreticalRisksTabular1978}{1978}).
It turns out, if one fits the same statistical model as published on newly gathered data, one fails to achieve the same results as published more often than not (Open Science Collaboration, \protect\hyperlink{ref-opensciencecollaborationEstimatingReproducibilityPsychological2015}{2015}).
Such failure to replicate findings previously believed to be robust has amounted to a level some social scientists term a crisis.
They put forth various causes and remedies to this crisis.
Most remedies share a common theme: transparency.
Some call for Bayesian statistics (Maxwell et al., \protect\hyperlink{ref-maxwellPsychologySufferingReplication2015}{2015}), as it makes assumptions more explicit, or demand preregistration (Nosek et al., \protect\hyperlink{ref-nosekPreregistrationRevolution2018}{2018}) as a means to clarify how to analyse the data, beforehand and publicly, others require the researchers to publish their data (Boulton et al., \protect\hyperlink{ref-boultonScienceOpenEnterprise2012}{2012}).
Similar calls for transparency, as a response to the replication crises, have formed the open science movement which stresses the necessity of six principles (Kraker et al., \protect\hyperlink{ref-krakerCaseOpenScience2011}{2011}):

\begin{itemize}
\tightlist
\item
  Open Access
\item
  Open Data
\item
  Open Source
\item
  Open Methodology
\item
  Open Peer Review
\item
  Open Educational Resources
\end{itemize}

I argue that a research product resting on these pillars optimally facilitates replication and hence satisfies the highest standard of reproducibility.
If everyone has access to a scientific product and its data along with the source code, leading them to understand the methodology and thus enabling them to criticise the result and educate themself, one is in the best position to replicate it.
Hence, any one's ability to reproduce such result gives a tangible affirmation of its usefulness to the scientific community.

However, reproducibility is no hurdle when anyone can perform the calculations needed with a pocket calculator; however, the more and more frequent use of computer-intensive methods renders such expectation questionable.
The use of machine learning techniques, once enabled by the computer taking over strenuous works, now impedes our quest for reproducibility.
More massive amounts of more complicated computer code than ever create room for errors and misunderstandings, leading the machine learning community to believe that they face a reproducibility crisis themselves (Hutson, \protect\hyperlink{ref-hutsonArtificialIntelligenceFaces2018}{2018}).
Yet, I am far from calling for abstinence from machine learning, just because it complicates reproduction, but want to emphasise the need for solutions that allow anyone to reproduce even the most sophisticated analysis.

Peikert \& Brandmaier (\protect\hyperlink{ref-peikertReproducibleDataAnalysis2019}{2019}) put forth an analysis workflow which provides this accessibility for everyone to reproduce any analysis.
However, they fail to provide the same level of convenience for the researcher who created an analysis in the first place.
Setting up the workflow eats up a considerable amount of the researcher's time, which they may better spend at advancing research.
This additional effort offsets the increase in productivity, promised by reproducibility, which I regard as most significant in the workflows adoption.
Persuading researchers, who find the meta-scientific argumentation noble but impractical, do not care about it or oppose it, requires concrete, practical benefits.
Luckily, most of this setup process may be automated, letting the researcher enjoy the workflows advantages while decreasing the efforts necessary to achieve them.
Providing an easier to use and more accessible version of the analysis workflow by Peikert \& Brandmaier (\protect\hyperlink{ref-peikertReproducibleDataAnalysis2019}{2019}) is the goal of this thesis and the herein presented \texttt{repro}-package for the R programming language (Peikert et al., \protect\hyperlink{ref-R-repro}{2020}).

\hypertarget{technical-solutions}{%
\section{Technical Solutions}\label{technical-solutions}}

This section summarises the workflow proposed by Peikert \& Brandmaier (\protect\hyperlink{ref-peikertReproducibleDataAnalysis2019}{2019}; see also The Turing Way Community et al., \protect\hyperlink{ref-theturingwaycommunityTuringWayHandbook2019}{2019} for a very similar approach).
They argue that to ensure reproducibility, publically sharing code is not enough.
Instead, reproducibility has to rest on five pillars:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \begin{description}
  \tightlist
  \item[file management]
  a folder containing all files, referring to each other using relative paths
  \end{description}
\item
  \begin{description}
  \tightlist
  \item[literate programming]
  a central dynamic document, that relates code to thought
  \end{description}
\item
  \begin{description}
  \tightlist
  \item[version control]
  a system in place that manages revisions of all files over time
  \end{description}
\item
  \begin{description}
  \tightlist
  \item[dependency management]
  a formal description of how files relate to each other
  \end{description}
\item
  \begin{description}
  \tightlist
  \item[containerization]
  an exact specification of the computational environment
  \end{description}
\end{enumerate}

These pillars stipulate the relations between thought, code and data with their change over time and environment and hence reach all requirements of reproducibility through being:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  understandable to other researchers,
\item
  transferable across machines,
\item
  conserved through time.
\end{enumerate}

While comprehensibility to the scientific community, is probably the most crucial goal, it is the most difficult to achieve.
That is because as a non-technical requirement, no set of rules may assure its fulfilment (though clear writing\footnote{Williams (\protect\hyperlink{ref-williamsStyleLessonsClarity2017}{2017}) provides some excellent principles for writing clearly.} and clean code\footnote{Martin (\protect\hyperlink{ref-martinCleanCoderCode2011}{2011}) proposes a coding paradigm that found widespread use because of its focus on understandability.} certainly help).
Transfer and conservation, on the other hand, are problems with technical solutions.
Peikert \& Brandmaier (\protect\hyperlink{ref-peikertReproducibleDataAnalysis2019}{2019}) propose to use a combination of RMarkdown, Git, Make, and Docker, because they are the most popular solutions for users of the R programming language (R Core Team, \protect\hyperlink{ref-R-base}{2020}).
However, they stress that any combination of tools is suitable as long as it facilitates the above pillars.

Each of the following sections first raises a challenge for reproducibility, then outlines a conceptual remedy along with a concrete tool and concludes how they relate to the package \texttt{repro}. The relation of these tools with \texttt{repro} is then expanded in the next chapter.

\hypertarget{file-organisation}{%
\subsection{File Organisation}\label{file-organisation}}

File organization has to meet two challenges. First, the structure needs to be understandable by others, and second, it needs to be self-contained so that it can be moved to another machine.

Adhering to conventions help other people understand how files are organized.
For example, the filename \texttt{R/reshape.R} follows both standard naming conventions (all lowercase, ends with .R, placed within the R directory) and is meaningful. Contrarily, \texttt{myScripts/munge\_Data.r} is probably a lot harder to understand and remember for most R-users.

Following two guidelines makes the file structure self-contained:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Everything is in one folder.
\item
  Every path is relative to that folder.
\end{enumerate}

This simple concept of a self-contained folder is facilitated by two R specific tools, \href{https://r4ds.had.co.nz/workflow-projects.html}{RStudio projects} and the \texttt{here} package (Müller, \protect\hyperlink{ref-R-here}{2017}).
The former frees the user from changing the working directory manually; the latter infers absolute paths from relative ones.
However, unlike the native R solution, this inference is consistent across operating systems and scripts and RMarkdowns.

The \texttt{repro} package comes with a template for an RStudio Project, which sets up a file structure that follows best practices and conventions.

\hypertarget{dynamic-document-generation}{%
\subsection{Dynamic Document Generation}\label{dynamic-document-generation}}

Even when following the most logical structure, it may be difficult for a reader of a scientific document to understand how the content of the document relates to the alongside published code.
Providing a direct link, dynamic document generation allows interspersing text with code and its results, producing one human-readable document.
The key feature is that every time such a document is rerun, the results are reproduced dynamically.
This functionality eliminates errors due to copying and pasting results from statistical software to a text processor. This mistake may be all too common; Nuijten et al. (\protect\hyperlink{ref-nuijtenPrevalenceStatisticalReporting2016}{2016}) reports that 50\% of papers from the psychological sciences contain an error that may be prevented.

RMarkdown does provide a convenient framework to write such dynamic documents and render them as a wide range of output formats\footnote{The document you are viewing also results from a collection of \href{https://github.com/aaronpeikert/repro-thesis}{RMarkdowns} availible as \href{https://aaronpeikert.github.io/repro-thesis/}{website}, \href{https://aaronpeikert.github.io/repro-thesis/ma.pdf}{PDF} and \href{https://aaronpeikert.github.io/repro-thesis/ma.epub}{E-book}}. In an RMarkdown, three parts can be distinguished:

\begin{itemize}
\tightlist
\item
  one specifying its output and metadata,
\item
  one containing code, and
\item
  one with descriptive text.
\end{itemize}

Each part uses its own language, all of them designed with ease of use and readability in mind.
The section containing the output format and other metadata alongside is written in YAML (see the example below).
This specification is located at the top, separated by three dashes at the beginning and end of the section.
(R-)Code executing an analysis can be placed in a distinct chunk or inline within the text.
The former has three backticks on their own line signifying beginning and end.
The later is quoted in a pair single backticks.
Examples of both methods can be found below.
Text, which is not fenced by either three dashes or backticks, is interpreted as literal text written in the Markup language ``Markdown''.
Markdown allows annotating text to signify formattings such as bold, italic, links and the inclusion of images.

The following section shows examples of metadata, code and text, specified as above described, forming a minimal example of an RMarkdown (\href{https://github.com/rstudio/rmarkdown-book/blob/a10b33d47a2b223a8ef643c245d45e4dfc7091b8/02-basics.Rmd\#L15-L39}{adapted source code} from Xie et al. (\protect\hyperlink{ref-xieMarkdownDefinitiveGuide2019}{2019})/\href{https://creativecommons.org/licenses/by-nc-sa/4.0/}{CC BY-NC-SA 4.0}):

\begin{Shaded}
\begin{Highlighting}[]
\OtherTok{---}
\FunctionTok{title:}\AttributeTok{ }\StringTok{"Hello R Markdown"}
\FunctionTok{author:}\AttributeTok{ }\StringTok{"Ross Ihaka & Robert Gentleman"}
\FunctionTok{date:}\AttributeTok{ }\StringTok{"1997-04-23"}
\FunctionTok{output:}\AttributeTok{ pdf_document}
\OtherTok{---}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{This is a paragraph in an R Markdown document.}

\NormalTok{Below is a code chunk:}

\NormalTok{```\{r\}}
\NormalTok{fit = lm(dist ~ speed, data = cars)}
\NormalTok{b   = coef(fit)}
\NormalTok{plot(cars)}
\NormalTok{abline(fit)}
\NormalTok{```}

\NormalTok{The slope of the regression is }\BaseNTok{`r b[1]`}\NormalTok{.}
\end{Highlighting}
\end{Shaded}

Resulting in this document:

{[}fixme{]}

The \texttt{repro} package extends the yaml metadata to incorporate \protect\hyperlink{dependency-management}{Dependency Management} and \protect\hyperlink{containarization}{Containarization} into the process of dynamic document creation.

\hypertarget{version-control}{%
\subsection{Version Control}\label{version-control}}

Text, code and results of a scientific document are refined in cycles of many revisions to accommodate highest standards.
As changes accumulate, different versions do too, posing a problem for reproducibility as it may be challenging to find out which version of code relates to the final product.
One may argue that in the typical publication process, the final product is apparent: the published paper, so only one version is relevant.
However, reproducibility may be crucial even before publication as part of the peer-review process.
Also, recent trends in the publication process like preprints, open review, registered reports and post-publication review, blur the lines between published and unpublished.

To organize different versions as changes accumulate across the phases of a project across machines and users is a well-known challenge in software development.
This challenge is met by a high degree of automation that keeps track of different versions and has advanced facilities to compare and merge them.

One such version control software is Git. Git tracks versions of the project folder by taking snapshots of a given state called commits.
Each commit has a unique id, called a hash, a short description of the changes made, called commit message and a link to the previous commit.
This linking creates a ``pedigree'' of versions where it is easy to see how things have evolved.
Going back in time to a specific version only requires to know the hash of the commit.
To mark commits as special milestones, they can be tagged, e.g.~as preregistration, preprint, submission or publication.

While mastering Git requires some experience, most of the time, only four commands are needed, which may be accessed through RStudio's Git interface:

\begin{description}
\tightlist
\item[git add]
take a snapshot of the given file
\item[git commit]
create a commit of all added files
\item[git push]
upload recent commits to a server
\item[git pull]
download and integrate recent commits from the server
\end{description}

While a few other commands are necessary to set up Git in a given project directory, this work is done by the \texttt{repro}-package.

\hypertarget{dependency-management}{%
\subsection{Dependency Management}\label{dependency-management}}

In an analysis, the results depend on code which in turn depends on data.
However, seldomly the data is analyzed as is, but some code is dedicated to preparing it.
Most likely, each analysis needs a slightly different version of the data.
An analysis of missingness requires the missings to be retained, but some statistical models do not allow that.
Or the modelling software requires data to be differently shaped, then the plotting library.
It is also often the case that one analysis is based on the output of another and so forth.
As these relations can become quite complicated, it is necessary to make them explicit to avoid confusion.
Dependency management provides a formalism that describes how files depend on other files.
More specifically, it provides an automated way to create files from other files, e.g.~it automatically generates a cleaned version of the data, by relying on a cleaning script and the raw data.
Such relations may be layered; hence, if a plot requires this cleaned dataset, first the cleaned dataset and then the plot is generated automatically.
Such structure allows saving considerable computing time, as dependencies are not generated again if they already exist, but only if one of their dependencies has changed.
In this example, upon recreation of the plot, the cleaned dataset is not generated as long as the cleaning script and the raw data remain unchanged.
Such intelligent behaviour is most useful when the preprocessing requires a lot of computing time as is typical in neuroimaging or machine learning.

Make is a tool for dependency management, while originally designed for the compilation of programs, it is now increasingly recognized as a tool for reproducibility.
It allows for all features above and more as it is an own programming language.

However, the repro package provides a much-simplified interface to the essential features, eschewing the need to learn yet another language.

\hypertarget{containarization}{%
\subsection{Containarization}\label{containarization}}

Most computer code is not self-contained but needs libraries and other software to work (e.g.~the R programming language or packages).
These external dependencies pose a risk for reproducibility because it may not be clear what besides the code and data is necessary and how to install it.
Even when all needed software and their exact versions are recorded meticulously, it may be a challenge to install them.
First, it is difficult to maintain different software versions on the same computer and second it may be unclear how to obtain an exact copy of some years old software version.
Setting up a computer exactly as someone else is difficult enough, but replicating some other computer how it was years ago is at best painstaking.

To overcome this challenge, the software environment of a project needs separation from the rest of the software environment. Technically such separation is called virtualization because one software environment is hosted on another. Such virtual environment allows each project to have its own software environment without interfering with each other. Hence, such setup is ideal for conservation and can be easily recreated on another machine.

Docker allows virtualization of the whole software stack down to the operating system, but in a much more lightweight way than traditional virtual machines. This lightweight but comprehensive virtualization is called containerization. Containers save storage by being based on each other enabling reuse. Hence if two containers are based, e.g.~on a container for the same R version, they only use the storage they need for the different R packages. Containers are created from a plain specification called \texttt{Dockerfile}, that defines on which container it should be based and what software should be installed within it.

The \texttt{repro}-package automatically infers which packages are needed and creates an appropriate Dockerfiles and the container from it.

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\leavevmode\hypertarget{ref-andersonScientificMethod2016}{}%
Andersen, H., \& Hepburn, B. (2016). Scientific method. In E. N. Zalta (Ed.), \emph{The stanford encyclopedia of philosophy} (Summer 2016). \url{https://plato.stanford.edu/archives/sum2016/entries/scientific-method/}; Metaphysics Research Lab, Stanford University.

\leavevmode\hypertarget{ref-AnnouncementReducingOur2013}{}%
Announcement: Reducing Our Irreproducibility. (2013). \emph{Nature}, \emph{496}(7446), 398--398. \url{https://doi.org/10.1038/496398a}

\leavevmode\hypertarget{ref-boultonScienceOpenEnterprise2012}{}%
Boulton, G., Campbell, P., Collins, B., Elias, P., Hall, W., Laurie, G., O'Neill, O., Rawlins, M., Thornton, J., \& Vallance, P. (2012). Science as an open enterprise. \emph{The Royal Society}.

\leavevmode\hypertarget{ref-claerboutElectronicDocumentsGive1992}{}%
Claerbout, J. F., \& Karrenbach, M. (1992). Electronic documents give reproducible research a new meaning. \emph{SEG Technical Program Expanded Abstracts 1992}, 601--604. \url{https://doi.org/10.1190/1.1822162}

\leavevmode\hypertarget{ref-dfg2019}{}%
Deutsche Forschungsgemeinschaft. (2019). \emph{Leitlinien zur Sicherung guter wissenschaftlicher Praxis}. \url{https://www.dfg.de/download/pdf/foerderung/rechtliche_rahmenbedingungen/gute_wissenschaftliche_praxis/kodex_gwp.pdf}

\leavevmode\hypertarget{ref-epskamp2019rep}{}%
Epskamp, S. (2019). Reproducibility and replicability in a fast-paced methodological world. \emph{Advances in Methods and Practices in Psychological Science}, \emph{2}(2), 145--155. \url{https://doi.org/https://doi.org/10.1177/2515245919847421}

\leavevmode\hypertarget{ref-gigerenzerNullRitualWhat2004}{}%
Gigerenzer, G., Krauss, S., \& Vitouch, O. (2004). The Null Ritual: What You Always Wanted to Know About Significance Testing but Were Afraid to Ask. In D. Kaplan, \emph{The SAGE Handbook of Quantitative Methodology for the Social Sciences} (pp. 392--409). SAGE Publications, Inc. \url{https://doi.org/10.4135/9781412986311.n21}

\leavevmode\hypertarget{ref-gilbertModelBuildingDefinition1991}{}%
Gilbert, S. W. (1991). Model building and a definition of science. \emph{Journal of Research in Science Teaching}, \emph{28}(1), 73--79. \url{https://doi.org/10.1002/tea.3660280107}

\leavevmode\hypertarget{ref-heilbronOxfordCompanionHistory2004}{}%
Heilbron, J. L. (Ed.). (2004). The Oxford Companion to the History of Modern Science. \emph{Reference Reviews}, \emph{18}(4), 40--41. \url{https://doi.org/10.1108/09504120410535443}

\leavevmode\hypertarget{ref-hutsonArtificialIntelligenceFaces2018}{}%
Hutson, M. (2018). Artificial intelligence faces reproducibility crisis. \emph{Science}, \emph{359}(6377), 725--726. \url{https://doi.org/10.1126/science.359.6377.725}

\leavevmode\hypertarget{ref-ioannidisWhyMostPublished2005}{}%
Ioannidis, J. P. A. (2005). Why Most Published Research Findings Are False. \emph{PLOS Medicine}, \emph{2}(8), e124. \url{https://doi.org/10.1371/journal.pmed.0020124}

\leavevmode\hypertarget{ref-jordanMachineLearningTrends2015}{}%
Jordan, M. I., \& Mitchell, T. M. (2015). Machine learning: Trends, perspectives, and prospects. \emph{Science}, \emph{349}(6245), 255--260. \url{https://doi.org/10.1126/science.aaa8415}

\leavevmode\hypertarget{ref-krakerCaseOpenScience2011}{}%
Kraker, P., Leony, D., Reinhardt, W., Gü, N., \& Beham, nter. (2011). The case for an open science in technology enhanced learning. \emph{International Journal of Technology Enhanced Learning}, \emph{3}(6), 643. \url{https://doi.org/10.1504/IJTEL.2011.045454}

\leavevmode\hypertarget{ref-martinCleanCoderCode2011}{}%
Martin, R. C. (2011). \emph{The clean coder: A code of conduct for professional programmers / Robert C. Martin} (1. print.). Prentice Hall.

\leavevmode\hypertarget{ref-maxwellPsychologySufferingReplication2015}{}%
Maxwell, S. E., Lau, M. Y., \& Howard, G. S. (2015). Is psychology suffering from a replication crisis? What does ``failure to replicate'' really mean? \emph{American Psychologist}, \emph{70}(6), 487.

\leavevmode\hypertarget{ref-meehlAppraisingAmendingTheories1990}{}%
Meehl, P. E. (1990). Appraising and Amending Theories: The Strategy of Lakatosian Defense and Two Principles that Warrant It. \emph{Psychological Inquiry}, \emph{1}(2), 108--141. \url{https://doi.org/10.1207/s15327965pli0102_1}

\leavevmode\hypertarget{ref-meehlTheoreticalRisksTabular1978}{}%
Meehl, P. E. (1978). Theoretical risks and tabular asterisks: Sir Karl, Sir Ronald, and the slow progress of soft psychology. \emph{Journal of Consulting and Clinical Psychology}, \emph{46}(4), 806--834. \url{https://doi.org/10.1037/0022-006X.46.4.806}

\leavevmode\hypertarget{ref-R-here}{}%
Müller, K. (2017). \emph{Here: A simpler way to find your files}. \url{https://CRAN.R-project.org/package=here}

\leavevmode\hypertarget{ref-nosekPreregistrationRevolution2018}{}%
Nosek, B. A., Ebersole, C. R., DeHaven, A. C., \& Mellor, D. T. (2018). The preregistration revolution. \emph{Proceedings of the National Academy of Sciences}, \emph{115}(11), 2600--2606. \url{https://doi.org/10.1073/pnas.1708274114}

\leavevmode\hypertarget{ref-nuijtenPrevalenceStatisticalReporting2016}{}%
Nuijten, M. B., Hartgerink, C. H. J., van Assen, M. A. L. M., Epskamp, S., \& Wicherts, J. M. (2016). The prevalence of statistical reporting errors in psychology (1985--2013). \emph{Behavior Research Methods}, \emph{48}(4), 1205--1226. \url{https://doi.org/10.3758/s13428-015-0664-2}

\leavevmode\hypertarget{ref-opensciencecollaborationEstimatingReproducibilityPsychological2015}{}%
Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. \emph{Science}, \emph{349}(6251), aac4716--aac4716. \url{https://doi.org/10.1126/science.aac4716}

\leavevmode\hypertarget{ref-peikertReproducibleDataAnalysis2019}{}%
Peikert, A., \& Brandmaier, A. M. (2019). \emph{A Reproducible Data Analysis Workflow with R Markdown, Git, Make, and Docker} {[}Preprint{]}. PsyArXiv. \url{https://doi.org/10.31234/osf.io/8xzqy}

\leavevmode\hypertarget{ref-R-repro}{}%
Peikert, A., Brandmaier, A. M., \& van Lissa, C. J. (2020). \emph{Repro: Easy setup of a reproducible workflow}. \url{https://github.com/aaronpeikert/repro}

\leavevmode\hypertarget{ref-popperCommentsTruthGrowth1962}{}%
Popper, K. R. (1962). Some comments on truth and the growth of knowledge. In E. Nagel, P. Suppes, \& A. Tarski (Eds.), \emph{Logic, Methodology and Philosophy of Science Proceedings of the 1960 International Congress} (Vol. 155). Stanford University Press.

\leavevmode\hypertarget{ref-R-base}{}%
R Core Team. (2020). \emph{R: A language and environment for statistical computing}. R Foundation for Statistical Computing. \url{https://www.R-project.org/}

\leavevmode\hypertarget{ref-rodgersEpistemologyMathematicalStatistical2010}{}%
Rodgers, J. L. (2010). The epistemology of mathematical and statistical modeling: A quiet methodological revolution. \emph{American Psychologist}, \emph{65}(1), 1--12. \url{https://doi.org/10.1037/a0018326}

\leavevmode\hypertarget{ref-theturingwaycommunityTuringWayHandbook2019}{}%
The Turing Way Community, Arnold, B., Bowler, L., Gibson, S., Herterich, P., Higman, R., Krystalli, A., Morley, A., O'Reilly, M., \& Whitaker, K. (2019). \emph{The Turing Way: A Handbook for Reproducible Data Science}. \url{https://doi.org/10.5281/zenodo.3233986}

\leavevmode\hypertarget{ref-tichyVerisimilitudeRedefined1976}{}%
Tichỳ, P. (1976). Verisimilitude redefined. \emph{The British Journal for the Philosophy of Science}, \emph{27}(1), 25--42.

\leavevmode\hypertarget{ref-williamsStyleLessonsClarity2017}{}%
Williams, J. M. (2017). \emph{Style: Lessons in clarity and grace} (Twelfth Edition). Pearson.

\leavevmode\hypertarget{ref-xieMarkdownDefinitiveGuide2019}{}%
Xie, Y., Allaire, J. J., \& Grolemund, G. (2019). \emph{R Markdown: The definitive guide}.

\end{document}
