# Outlook

While the `repro` package exports `r length(ls("package:repro"))` well-documented functions, assures their correctness by close to 200 unit tests and amasses all in all short of 2000 lines of code, many workflows and features are incomplete.
I hope to make progress in three areas:

1. Building an infrastructure for other workflows,
2. explore "continues integration" (CI) for research projects, and
3. enable the leverage of high-performance computing (HPC) clusters and cloud infrastructure.

Because the user interface of `repro` was modeled after the `usethis` package, `repro` also inherited `usethis`' modular structure.
This design decision was made more by accident than by intention but is in hindside more than useful.
Because of its modularity, `repro` can be extended easily and can serve as an infrastructure package.
This use-case will be explored in collaboration with Caspar van Lissa, who proposes a workflow, called [`worcs`](https://cjvanlissa.github.io/worcs/) [@vanlissaWORCSWorkflowOpen2020], which is similar in spirit, but uses different tools.
[`worcs`](https://cjvanlissa.github.io/worcs/) also utilizes RMarkdown and Git, but in place of Docker, it uses [`renv`](https://rstudio.github.io/renv/articles/renv.html) and instead of Make it relies on a highly standardized file structure.
Hence, only two "moduls" or rather functions have to be added to `repro` to support the workflow, `use_renv()` (which replaces `use_docker()`) and `use_worcs_template()` (which replaces `use_repro_template()`).
Also, [`worcs`](https://cjvanlissa.github.io/worcs/) has other features like automatic codebook creation and synthetic data generation, which may be excellent supplements for a `repro` project.
To ensure interoperability between the packages, it is planned that much of the behind the scenes machinery is moved from [`worcs`](https://cjvanlissa.github.io/worcs/) to `repro` in a more modular form.
However, the users will not notice much of a difference, except that they may fuse both workflows into one.

Such lego system of reproducibility tools, where the users can decide which tools they want to include may be especially useful when some features are not strictly necessary for reproducibility.
In its current form, no tool is optional in the sense that only in unison they guarantee reproducibility.
However, some features may be useful but not necessary for reproducibility.

For example, is it useful to have an online service such as GitHub Actions that asserts reproducibility of each change, but it is by no means necessary.
As previously discussed there currently exists no easy solution to verify reproduction automatically.
The key difficulty is to detect changes in the results that potentially alter the conclusion, from those which carry no meaning, e.g. if the date is updated.
To address this challenge, I currently work on a prototype of an R package, which allows the researcher to assert that some objects remain unchanged upon reproduction, by wrapping them into `unchanged()`.
This package keeps track of thus marked objects and throws an error if they accidentally change.
If such a solution would exist, there would be no hurdle to incorporate continues integration into research projects.
This would mean that all contributions are automatically vetted before they are incorporated and that the results are automatically updated, e.g. the preprint or the additional material on [osf.org](osf.org) would automatically be re-uploaded.
Then projects could have a badge that signifies that a third party is able to reproduce it.
Such continues updates do not only ease collaboration but are also especially interesting for some continues data sources, e.g. long-running developmental studies or meta-analysis, where new data becomes repeatedly available even after publication.

Conveniences such as the above are possible because the here presented conception of reproducibility is highly automated.
Hence, an analysis may be moved to another machine without manual intervention.
This also opens some interesting possibilities unrelated to reproducibility, concerning the management of computational resources.
It is often the case that an analysis requires substantial amounts of computational resources, more than a single computer may deliver.
In such a case, a here described analysis can easily be moved to a more powerful computer or even spread across hundreds.
The use of containers is the de facto standard of cloud computing providers but also becomes increasingly common for high-performance clusters.
Hence it is a task that can easily be accomplished with `repro`.
However, these functions are still in development and not tested well enough to be published.
A typical problem, when utilizing distributed computation, is the division of tasks between the computing instances.
However, this is pretty straightforward because of the deployed dependency management.
Make can distribute tasks across thousands of nodes while making sure that none of the dependencies collide.
Initially, this functionality was available in `repro` for TORQUE, an HPC task scheduler, but because TORQUE will no longer be maintained I will phase out this set of functions.

`repro` strives to make reproducibility more attractive by lowering the barriers to advanced reproducibility tools.
To my knowledge, no standard is as comprehensive as the here described combination of Git, RMarkdown, Make and Docker.
However, the workflow presented by @vanlissaWORCSWorkflowOpen2020 sacrifices a little bit of this flexibility to reduce the complexity drastically.
Despite these efforts to make reproducibility easier, I think it is worthwhile to simplify the approaches further, to appeal to users that are more comfortable using Microsoft Office than RMarkdown and hesitant to use a formal version control system.
Hopefully, `repro` is a first step to make reproducibility easier to archive.
