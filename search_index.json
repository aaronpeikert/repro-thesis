[
["technical-solutions.html", "Chapter 2 Technical Solutions 2.1 File Organisation 2.2 RMarkdown 2.3 Git 2.4 Make 2.5 Docker", " Chapter 2 Technical Solutions This section summarises the workflow proposed by Peikert &amp; Brandmaier (2019; see also The Turing Way Community et al., 2019 for a very similar approach). They argue that to ensure reproducibility, publically sharing code is not enough. Instead, reproducibility has to rest on five pillars: file management a folder containing all files, referring to each other using relative paths literate programming a central dynamic document, that relates code to thought version control a system in place that manages revisions of all files over time dependency management a formal description of how files relate to each other containerization an exact specification of the computational environment These pillars specify the relations between thought, code and data along with their change over time and environment unambiguesly and hence meet all requirements of reproducibility: understandable by other researcher transferable across machine conserved through time While comprehensibility to the scientific community, is probably the most important goal, it is the most difficult to achieve. That is because as a non-technical requirement, no set of rules may assure its fulfilment (though clear writing1 and clean code2 certainly help). Transfer and conservation, on the other hand, are problems with technical solutions. Peikert &amp; Brandmaier (2019) propose to use a combination of RMarkdown, Git, Make, and Docker, because they are the most popular solutions for users of the R programming language (R Core Team, 2020). However, they stress that any combination of tools is suitable as long as it facilitates the above pillars. 2.1 File Organisation There is only one primary guideline for organizing files: all files need to be moveable to another place. This implies two rules to follow: Everything in one folder. Every path relative to that folder. This simple concept of a “project” folder is facilitated by two R specific tools, RStudio projects and the here package (Müller, 2017). The former frees the user from changing the working directory manually; the latter infers absolute paths from relative ones. However, unlike the native R solution, it does so consistently across operating systems and across scripts and RMarkdowns. Using meaningful names and adhering to conventions help other people understand how files are organized. For example, the filename R/reshape.R follows both standard naming conventions (all lowercase, ends with .R, placed within the R directory) and is meaningful, while myScripts/munge_Data.r is probably a lot harder to understand and remember for most R-users. 2.2 RMarkdown Even with the most logical structure, it may be difficult for a reader of a scientific document to understand how the content of the document relates to the alongside published code. Providing a direct link, RMarkdown allows interspersing text with code and its results producing a wide range of output formats3. The key feature is that every time an RMarkdown is rerun, the results are reproduced dynamically. This functionality eliminates errors due to copying and pasting results from statistical software to a text processor, a mistake that may be all too common (Nuijten et al., 2016 reports that 50% of papers from the psychological sciences contain an error that may be prevented). In an RMarkdown three parts can be distinguished: one specifying its output, one containing code and one with descriptive text. Each part uses its own language, all of them designed with ease of use and readability in mind. The section containing the output format and other metadata alongside is written in YAML (see the example below). This specification is located at the top and separated by three dashes at the beginning and end of the section. (R-)Code executing an analysis can be placed in a distinct chunk or inline within the text. The former has three backticks on their own line signifying beginning and end. The later is quoted in a pair single backticks. Examples of both methods can be found below. Text, which is not fenced by either three dashes or backticks, is interpreted as literal text written in the Markup language “Markdown”. Markdown allows annotating text to signify formattings such as bold, italic, links and the inclusion of images. The following section shows examples of metadata, code and text, specified as above described, forming a minimal example of an RMarkdown (adapted source code from Xie et al. (2019)/CC BY-NC-SA 4.0): --- title: &quot;Hello R Markdown&quot; author: &quot;Awesome Me&quot; date: &quot;2018-02-14&quot; output: html_document --- This is a paragraph in an R Markdown document. Below is a code chunk: ```{r} fit = lm(dist ~ speed, data = cars) b = coef(fit) plot(cars) abline(fit) ``` The slope of the regression is `r b[1]`. Resulting in this document: [fixme] 2.3 Git Text, code and results of a scientific document are refined in cycles of many revisions to accomodate highest standards. As changes accumulate, different versions do too, posing a problem for reproducibility as it may be difficult to find out which version of code relates to the final product. One may argue that in the typical publication process the final product is obvius: the published paper, so only one version is important. However, reproduction may be crucial even before publication as part of the peer review process. Also recent trends in the publication process like preprints, open review, registered reports and post publication review, blur the lines between published and unpublished. Git tracks versions of the project folder by making snapshots of a given state, called commits. Each commit has an unique id, called hash, a short description of the changes made, called commit message and a link to the previus commit. This creates a “pedigree” of versions where it is easy to see how things have evolved. Going back in time to a specific version only requires to know the hash of the commit. To mark commits as special milestones they can be tagged e.g. as preregestration, preprint, submission or publication. While mastering Git requires some experience, most of the time only four commands are needed: git init use git in the current directory git add take snapshot of the given file git commit create a commit of all added files git push upload recent commits to a server git pull download and integrate recent commits from server While a few other commands are neccesary to set up Git in a given project directory, this work is done by the repro-package. 2.4 Make In an analysis the results depend on code which in turn depends on data. However, seldomly the data is analysed as is, but some code is dedicated to prepare it. Most likely each analysis needs a slightly different version of the data. An analysis of missingness requires the missings to be retained, but some statistical models do not allow it. Or the modelling software requires data to be differently shaped, then the plotting library. It is also often the case that one analysis is based on the output of another and so forth. As these relations can become quite complicated it is nessary to make them explicit to avoid confusion. Dependency management provides a formalism that descripes how files depend on other files. More specifically it provides an automated way to create files from other files, e.g. it automatically generates a cleaned version of the data, by relying on a cleaning script and the raw data. Such relations may be layered; hence, if a plot requires this cleaned dataset, first the cleaned dataset and then the plot are generated automatically. Such structure allows to save considerable computing time, as dependencies are not generated again if they allready exist, but only when one of their dependencies have changed. Hence in the example, upon recreation of the plot, the cleaned dataset is not again generated as long as neither the cleaning script nor the raw data have changed. Such clever behavior is most usefull when the preprosessing requires a lot computing time as is typical in neuroimaging or machine learning. Make is a tool for dependency management, while originally designed for the compilation of programs, it is now increasingly recognized as a tool for reproducibility. It allows for all features above and more as it is a own programming language. However, the repro package provides a much simplified interface to the most important features, echewing the need to learn yet another language. 2.5 Docker Most computer code is not self-contained, but needs libraries and other software to work (e.g. the R programming language or packages). This poses a risk for reproducibility because it may not be clear what besides the code and data is necessary and how to install it. Even when all needed software and their exact versions are recorded meticulously it may be a challange to install them. First, it is difficult to maintain different software versions on the same computer and second it may be unclear how to obtain an exact copy of some years old software version. Setting up a computer exactly as someonelses is difficult enough, but replicating some other computer how it was years ago is at best painstaking. To overcome this challange the software environment of a project needs to be separeted from the rest of the software environment. Technically such seperation is called virtualization, becouse one software environment is hosted on another. Such virtual envitonment allows each project to have its own software environment without interfeering with each other. Hence, such setup is ideal for conservation and can be easily recreated on another machine. Docker allows virtualization of the whole software stack down to the operating system, but in a much more lightweight way then traditional virtual machines. This lightweight but comprehensive virtualization is called containerization. Containers save storage by being based on each other enabling reuse. Hence if two containers are based e.g. on a container for the same R version, they only use the storage they need for the different R packages. Containers are created from a simple specification called Dockerfile, that defines on wich container it should be based and what software should be installed within it. The repro-package automatically infers which packages are needed and creates an appropriate Dockerfiles and the container from it. References "]
]
